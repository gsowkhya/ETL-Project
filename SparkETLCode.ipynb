{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ETL Project \n",
    "#### By Sowkhya Reddy Geeda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "os.environ[ \"PYSPARK_PYTHON\" ] = \"/opt/cloudera/parcels/Anaconda/bin/python\"\n",
    "os.environ[ \"JAVA_HOME\" ] = \"/usr/java/jdk1.8.0_161/jre\"\n",
    "os.environ[ \"SPARK_HOME\" ] = \"/opt/cloudera/parcels/SPARK2-2.3.0.cloudera2-1.cdh5.13.3.p0.316101/lib/spark2/\"\n",
    "os.environ[ \"PYLIB\" ] = os.environ[ \"SPARK_HOME\" ] + \"/python/lib\"\n",
    "sys.path.insert( 0 , os.environ[ \"PYLIB\" ] + \"/py4j-0.10.6-src.zip\" )\n",
    "sys.path.insert( 0 , os.environ[ \"PYLIB\" ] + \"/pyspark.zip\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Spark Session\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://ip-10-0-0-165.ec2.internal:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.3.0.cloudera2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>demo</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f35686507d0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Spark Session Creation\n",
    "spark = SparkSession.builder.appName('demo').master(\"local\").getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import necessary Library to define various data types for Schema Creation,\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, DoubleType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import necessary Library for customized date field creation,\n",
    "from pyspark.sql.functions import unix_timestamp\n",
    "from pyspark.sql.functions import to_timestamp\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Schema Preparation.\n",
    "fileSchema = StructType([StructField('year', IntegerType(),True),\n",
    "                         StructField('month', StringType(),True), \n",
    "                         StructField('day', IntegerType(),True), \n",
    "                         StructField('weekday', StringType(),True), \n",
    "                         StructField('hour', IntegerType(),True), \n",
    "                         StructField('atm_status', StringType(),True), \n",
    "                         StructField('atm_id', StringType(),True), \n",
    "                         StructField('atm_manufacturer', StringType(),True), \n",
    "                         StructField('atm_location', StringType(),True), \n",
    "                         StructField('atm_streetname', StringType(),True), \n",
    "                         StructField('atm_street_number', IntegerType(),True), \n",
    "                         StructField('atm_zipcode', IntegerType(),True), \n",
    "                         StructField('atm_lat', DoubleType(),True), \n",
    "                         StructField('atm_lon', DoubleType(),True), \n",
    "                         StructField('currency', StringType(),True), \n",
    "                         StructField('card_type', StringType(),True), \n",
    "                         StructField('transaction_amount', IntegerType(),True), \n",
    "                         StructField('service', StringType(),True), \n",
    "                         StructField('message_code', StringType(),True), \n",
    "                         StructField('message_text', StringType(),True), \n",
    "                         StructField('weather_lat', DoubleType(),True), \n",
    "                         StructField('weather_lon', DoubleType(),True), \n",
    "                         StructField('weather_city_id', IntegerType(),True), \n",
    "                         StructField('weather_city_name', StringType(),True), \n",
    "                         StructField('temp', DoubleType(),True), \n",
    "                         StructField('pressure', IntegerType(),True), \n",
    "                         StructField('humidity', IntegerType(),True), \n",
    "                         StructField('wind_speed', IntegerType(),True), \n",
    "                         StructField('wind_deg', IntegerType(),True), \n",
    "                         StructField('rain_3h', DoubleType(),True), \n",
    "                         StructField('clouds_all', IntegerType(),True), \n",
    "                         StructField('weather_id', IntegerType(),True), \n",
    "                         StructField('weather_main', StringType(),True), \n",
    "                         StructField('weather_description', StringType(),True),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Read Data from HDFS\n",
    "df = spark.read.csv(\"/user/root/SRC_ATM_TRANS/part-m-00000\",sep=\",\",  header=\"false\",  schema = fileSchema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the Count\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check1: Total Row Count Matches with Validation Sheet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Printing Schema\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Location Dimension Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create Location Dimension Dataframe\n",
    "DIM_LOCATION = df.select('atm_location','atm_streetname','atm_street_number','atm_zipcode','atm_lat','atm_lon').distinct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the recods.\n",
    "DIM_LOCATION.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "## No of distinct records present in the location Dimenstion dataframe.\n",
    "DIM_LOCATION.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import necessary library to generate surrogate Key\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import col,row_number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Check a column having any NULL value or not\n",
    "DIM_LOCATION.filter(DIM_LOCATION.atm_location.isNull()).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create location_id (primary key/surrogate Key)\n",
    "wi  =  Window().orderBy(\"atm_location\")\n",
    "DIM_LOCATION  =  DIM_LOCATION.select(row_number().over(wi).alias(\"location_id\"), col(\"*\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the data\n",
    "DIM_LOCATION.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "### creating a temporary table ######\n",
    "DIM_LOCATION.registerTempTable(\"DIM_LOCATION\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Check the count through SQL query\n",
    "spark.sql(\"select  count(*)  from  DIM_LOCATION\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check2: Location Dimension Count Matches with Validation sheet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Card Type Dimension Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Card Type Dimension Data Frame\n",
    "DIM_CARD_TYPE = df.select('card_type').distinct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create location_id (primary key/surrogate Key)\n",
    "wi = Window().orderBy(\"card_type\")\n",
    "DIM_CARD_TYPE = DIM_CARD_TYPE.select(row_number().over(wi).alias(\"card_type_id\"), col(\"*\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Displaying date in the Card Type Dataframe\n",
    "DIM_CARD_TYPE.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Checking the Count\n",
    "DIM_CARD_TYPE.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check3: Card Type Dimension Count Matches with Validation sheet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ATM Dimension Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Populate Data in ATM Dimension Dataframe\n",
    "DIM_ATM = df.select('atm_id','atm_manufacturer','atm_lat','atm_lon').distinct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Join (Left) ATM Dimension with Location Dimension since more ATM can be present acro ss different Latitude/Longitude\n",
    "DIM_ATM  =  DIM_ATM.join(DIM_LOCATION,on=['atm_lat','atm_lon'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check Count\n",
    "DIM_ATM.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check Schema of ATM Dimension\n",
    "DIM_ATM.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIM_ATM = DIM_ATM.select('atm_id','atm_manufacturer','location_id').distinct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check Count\n",
    "DIM_ATM.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Renaming the column\n",
    "DIM_ATM = DIM_ATM.withColumnRenamed(\"atm_id\",\"atm_number\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Original data = atm_id represents the atm_number\n",
    "\n",
    "For ATM DIM table, we need to create another column (atm_id) which will be used as Primary Key.\n",
    "\n",
    "Renaming the 'atm_id' column in ATM_dimension table' to 'atm_number' which will be used later during JOIN operation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking if any of the atm number is missing\n",
    "DIM_ATM.filter(DIM_ATM.atm_number.isNull()).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add Surrogate/PK key\n",
    "wi  =  Window().orderBy(\"atm_number\")\n",
    "DIM_ATM  =  DIM_ATM.select(row_number().over(wi).alias(\"atm_id\"),  col(\"*\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Display the content\n",
    "DIM_ATM.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Check The overall count\n",
    "DIM_ATM.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check4: ATM Dimension Count Matches with Validation sheet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DATE Dimension Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Populate data on Date Dimension Dataframe\n",
    "DIM_DATE = df.select('year','month','day','hour','weekday').distinct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing a temp column \"month numver (i.e. January = 01, February = 02) which will be used to create Full DateTime Data\"\n",
    "DIM_DATE = DIM_DATE.withColumn(\"month_number\",from_unixtime(unix_timestamp(col(\"month\"),'MMMM'),'MM'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate the  change\n",
    "DIM_DATE.show(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the Schema\n",
    "DIM_DATE.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Python Function (built-in functions available for DataFrame)\n",
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create customized \"full date time\" [desired format yyyy-MM-dd HH:mm:ss] \n",
    "DIM_DATE = DIM_DATE.withColumn('full_date_time',F.concat('year',F.lit('-'),'month_number',F.lit('-'),'day',F.lit(' '),'hour',F.lit(':00'),F.lit(':00')))\\\n",
    ".select(\"full_date_time\",\"year\",\"month\",\"day\",\"hour\",\"weekday\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIM_DATE.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Check numm Count\n",
    "DIM_DATE.filter(DIM_DATE.year.isNull()).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add PK key/Surrogate Key Date_id\n",
    "wi  =  Window().orderBy(\"year\")\n",
    "DIM_DATE  =  DIM_DATE.select(row_number().over(wi).alias(\"date_id\"),  col(\"*\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Change datatype for full_date_time (Timestamp format)\n",
    "pattern = \"yyyy-MM-dd HH:mm:ss\"\n",
    "DIM_DATE = DIM_DATE.withColumn(\"full_date_time\",unix_timestamp(\"full_date_time\", pattern).cast(\"timestamp\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Data value check\n",
    "DIM_DATE.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Schema check final\n",
    "DIM_DATE.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Count Check for Date Dimension\n",
    "DIM_DATE.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check5: DATE Dimension Count Matches with Validation sheet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Join and Create Fact Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create Stage 1 Data frame : Condition Original Data frame left join with Location Dimension DF with valid Keys\n",
    "stage_1 =  df.join(DIM_LOCATION,on=['atm_location','atm_streetname','atm_street_number','atm_zipcode','atm_lat','atm_lon'],how = 'left')\n",
    "stage_1.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check6: Total Row Count Matches with Validation sheet after Stage1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check Stage 1 schema\n",
    "stage_1.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename necessary column for Joining operation\n",
    "stage_1  =  stage_1.withColumnRenamed(\"atm_id\",\"atm_number\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Original data = atm_id represents the atm_number\n",
    "\n",
    "ATM DIM table, we have two columns 1. atm_id (contains row number) 2. atm_number = (atm_id value of original data)\n",
    "\n",
    "For Join, to create a matching key Renaming the 'atm_id' (original data) to 'atm_number' so original file and ATM dimenstion\n",
    "\n",
    "table can be joined with key = ['atm_number','atm_manufacturer','atm_location']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Print Schema of ATM Dimension\n",
    "DIM_ATM.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "##  Create Stage 2  Data frame: Condition : Stage 1 Data frame left join with ATM Dimension DF with valid Keys\n",
    "stage_2 = stage_1.join(DIM_ATM,on=['atm_number','atm_manufacturer','location_id'],how='left') \n",
    "stage_2.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check7: Total Row Count Matches with Validation sheet after Stage2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check Stage 2 Schema\n",
    "stage_2.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create Stage 3 Data frame : Condition : Stage 3 Data frame left join with Date Dimension DF with valid Keys\n",
    "stage_3 = stage_2.join(DIM_DATE,on=['year','month','day','hour','weekday'],how = 'left')\n",
    "stage_3.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check8: Total Row Count Matches with Validation sheet after Stage3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stage 3 Schema check\n",
    "stage_3.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "##  Create Stage 4  Dataframe : Condition: Stage 4 Data frame left join with Card Type Dimension DF with valid Keys\n",
    "stage_4 = stage_3.join(DIM_CARD_TYPE,on=['card_type'],how = 'left') \n",
    "stage_4.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check9: Total Row Count Matches with Validation sheet after Stage4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stage 4 Schema check\n",
    "stage_4.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fact Table Creation\n",
    "### Fact table with distinct() --- For Checking Purpose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Create Fact table based on required/mentioned column\n",
    "fact_table = stage_4.select('atm_id','location_id','date_id','card_type_id','atm_status','currency','service','transaction_amount','message_code',\n",
    "                            'message_text','rain_3h','clouds_all','weather_id','weather_main','weather_description').distinct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Add PK/Surrogate Key (Trans_id)\n",
    "wI  =  Window().orderBy(\"atm_id\")\n",
    "fact_table  =  fact_table.select(row_number().over(wI).alias(\"trans_id\"),  col(\"*\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "fact_table.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check10: Total Row Count Mismatch with Validation sheet for Fact Table with distinct()\n",
    "#### Check count for all the Stages in the creation of Transaction Fact table\n",
    "#### Count of Records in each stage - 2468572\n",
    "### Fact table without distinct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Create Fact table based on required/mentioned column\n",
    "FACT_ATM_TRANS = stage_4.select('atm_id','location_id','date_id','card_type_id','atm_status','currency','service','transaction_amount',\n",
    "                              'message_code','message_text','rain_3h','clouds_all','weather_id','weather_main','weather_description')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Add PK/Surrogate Key (Trans_id)\n",
    "wi  =  Window().orderBy(\"atm_id\")\n",
    "FACT_ATM_TRANS  =  FACT_ATM_TRANS.select(row_number().over(wi).alias(\"trans_id\"),  col(\"*\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count validation\n",
    "FACT_ATM_TRANS.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check11: Total Row Count Matches with Validation sheet for Fact Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Verify Fact Table Data\n",
    "FACT_ATM_TRANS.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We will proceed without Distinct as per Data validation reference spreadsheet\n",
    "#### Check count for the all the Stages in the creation of Transaction Fact table -\n",
    "#### Count of Records in each stage - 2468572"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create .csv file and store it HDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Save Location Dimension Dataframe in HDFS as .csv format\n",
    "DIM_LOCATION.write.format('csv').option('header',False).mode('overwrite').option('sep',',').save('s3a://etlproject-testbucket/DIM_LOCATION/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Save Card Type Dimension Dataframe in HDFS as .csv format \n",
    "DIM_CARD_TYPE.write.format('csv').option('header',False).mode('overwrite').option('sep', ',').save('s3a://etlproject-testbucket/DIM_CARD_TYPE/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Save ATM Dimension Dataframe in HDFS as .csv format\n",
    "DIM_ATM.write.format('csv').option('header',False).mode('overwrite').option('sep',',').save('s3a://etlproject-testbucket/DIM_ATM/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Save DATE Dimension Dataframe in HDFS as .csv format \n",
    "DIM_DATE.write.format('csv').option('header',False).mode('overwrite').option('sep', ',').save('s3a://etlproject-testbucket/DIM_DATE/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "##  Save Fact Table\tDataframe in HDFS as .csv format\n",
    "FACT_ATM_TRANS.write.format('csv').option('header',False).mode('overwrite').option('sep', ',').save('s3a://etlproject-testbucket/FACT_ATM_TRANS/')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
